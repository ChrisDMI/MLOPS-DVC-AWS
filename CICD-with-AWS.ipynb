{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cat ~/.aws/credentials\n",
    "### aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! You want to replicate the CI/CD pipeline described in the blog post, but using AWS services instead of Google Cloud Platform (GCP). Specifically, your model is stored in Amazon S3, and you want to configure DVC (Data Version Control) and GitHub Actions to work with AWS.\n",
    "\n",
    "Here’s how you can achieve this:\n",
    "\n",
    "\t1.\tConfigure DVC to use AWS S3 as the remote storage.\n",
    "\t2.\tSet up AWS credentials for local testing and in GitHub Actions.\n",
    "\t3.\tModify the Dockerfile to work with AWS S3.\n",
    "\t4.\tConfigure GitHub Actions to authenticate with AWS.\n",
    "\t5.\tUpdate the GitHub Actions workflow to build and test your Docker image.\n",
    "\n",
    "1. Configuring DVC to Use AWS S3 as Remote Storage\n",
    "\n",
    "First, you need to set up DVC to use Amazon S3 as the remote storage for your model artifacts.\n",
    "\n",
    "Install DVC with S3 Support:\n",
    "\n",
    "Make sure DVC is installed with S3 support:\n",
    "\n",
    "pip install \"dvc[s3]\"\n",
    "\n",
    "Add S3 Remote Storage:\n",
    "\n",
    "In your project directory, run:\n",
    "\n",
    "dvc remote add -d storage s3://your-bucket-name/path/to/remote\n",
    "\n",
    "\t•\tReplace your-bucket-name with your actual S3 bucket name.\n",
    "\t•\tReplace path/to/remote with the desired path in your bucket.\n",
    "\n",
    "Configure AWS Credentials for DVC:\n",
    "\n",
    "DVC will use AWS credentials to access the S3 bucket. You can configure credentials in several ways:\n",
    "\n",
    "\t•\tEnvironment Variables:\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=your_access_key_id\n",
    "export AWS_SECRET_ACCESS_KEY=your_secret_access_key\n",
    "export AWS_DEFAULT_REGION=your_region\n",
    "\n",
    "\n",
    "\t•\tAWS CLI Configuration:\n",
    "If you have the AWS CLI installed, you can run:\n",
    "\n",
    "aws configure\n",
    "\n",
    "\n",
    "\t•\tShared Credentials File (~/.aws/credentials):\n",
    "Add your credentials to the shared file.\n",
    "\n",
    "For local testing, setting up via aws configure or environment variables is sufficient.\n",
    "\n",
    "2. Setting Up AWS Credentials\n",
    "\n",
    "Create an AWS IAM User:\n",
    "\n",
    "\t•\tGo to the AWS IAM console.\n",
    "\t•\tCreate a new user (e.g., github-actions-user) with Programmatic Access.\n",
    "\t•\tAttach a policy that grants necessary permissions to access your S3 bucket.\n",
    "\n",
    "Example policy:\n",
    "\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"AllowS3Access\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:ListBucket\",\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:PutObject\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:s3:::your-bucket-name\",\n",
    "        \"arn:aws:s3:::your-bucket-name/*\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\t•\tReplace your-bucket-name with your actual bucket name.\n",
    "\n",
    "Store AWS Credentials as GitHub Secrets:\n",
    "\n",
    "\t•\tIn your GitHub repository, go to Settings > Secrets and variables > Actions > Secrets.\n",
    "\t•\tAdd the following secrets:\n",
    "\t•\tAWS_ACCESS_KEY_ID: Your IAM user’s Access Key ID.\n",
    "\t•\tAWS_SECRET_ACCESS_KEY: Your IAM user’s Secret Access Key.\n",
    "\t•\tAWS_DEFAULT_REGION: Your AWS region (e.g., us-east-1).\n",
    "\n",
    "3. Modifying the Dockerfile\n",
    "\n",
    "Adjust your Dockerfile to install DVC with S3 support and configure DVC to use your S3 remote.\n",
    "\n",
    "Example Dockerfile:\n",
    "\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set work directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy project files\n",
    "COPY . /app\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements_inference.txt\n",
    "RUN pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "# Initialize DVC\n",
    "RUN dvc init -f --no-scm\n",
    "\n",
    "# Configure DVC remote\n",
    "RUN dvc remote add -d storage s3://your-bucket-name/path/to/remote\n",
    "\n",
    "# Pull the trained model\n",
    "RUN dvc pull dvcfiles/trained_model.dvc\n",
    "\n",
    "# Expose port and run the application\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\n",
    "Important Notes:\n",
    "\n",
    "\t•\tDo not hardcode AWS credentials in the Dockerfile. Credentials will be provided through environment variables in GitHub Actions.\n",
    "\n",
    "4. Configuring GitHub Actions to Authenticate with AWS\n",
    "\n",
    "Use the aws-actions/configure-aws-credentials action to set up AWS credentials in your workflow environment.\n",
    "\n",
    "5. Updating the GitHub Actions Workflow\n",
    "\n",
    "Create or modify your workflow file in .github/workflows/ (e.g., ci-cd.yaml):\n",
    "\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout Code\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Configure AWS Credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v2\n",
    "        with:\n",
    "          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "\n",
    "      - name: Install Dependencies\n",
    "        run: |\n",
    "          pip install --no-cache-dir -r requirements_inference.txt\n",
    "          pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "      - name: Pull DVC Data\n",
    "        run: |\n",
    "          dvc pull dvcfiles/trained_model.dvc\n",
    "\n",
    "      - name: Build Docker Image\n",
    "        run: |\n",
    "          docker build -t inference:latest .\n",
    "\n",
    "      - name: Run Docker Container\n",
    "        run: |\n",
    "          docker run -d -p 8000:8000 --name inference_container inference:latest\n",
    "\n",
    "      - name: Test API Endpoint\n",
    "        run: |\n",
    "          sleep 5  # Wait for the server to start\n",
    "          curl -f http://localhost:8000/health\n",
    "\n",
    "      - name: Clean Up\n",
    "        run: |\n",
    "          docker stop inference_container\n",
    "          docker rm inference_container\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\t•\tCheckout Code: Retrieves your code from the repository.\n",
    "\t•\tConfigure AWS Credentials: Sets up AWS credentials for the workflow environment.\n",
    "\t•\tSet up Python: Ensures the correct Python version is used.\n",
    "\t•\tInstall Dependencies: Installs required Python packages, including DVC with S3 support.\n",
    "\t•\tPull DVC Data: Pulls the model from S3 using DVC.\n",
    "\t•\tBuild Docker Image: Builds the Docker image using your Dockerfile.\n",
    "\t•\tRun Docker Container: Starts the container.\n",
    "\t•\tTest API Endpoint: Checks if the API is running by hitting the /health endpoint.\n",
    "\t•\tClean Up: Stops and removes the Docker container to free resources.\n",
    "\n",
    "Notes:\n",
    "\n",
    "\t•\tThe aws-actions/configure-aws-credentials action exports AWS credentials as environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION), which are automatically used by DVC.\n",
    "\t•\tMake sure to replace your-bucket-name and other placeholders with your actual values.\n",
    "\t•\tAdjust working-directory if your Dockerfile is not in the root directory.\n",
    "\n",
    "Additional Considerations\n",
    "\n",
    "Security:\n",
    "\n",
    "\t•\tNever commit AWS credentials to your repository.\n",
    "\t•\tUse IAM policies to grant the minimum required permissions.\n",
    "\n",
    "DVC Tips:\n",
    "\n",
    "\t•\tEnsure that your .dvc files correctly reference the data files stored in S3.\n",
    "\t•\tAdd .dvc/cache and any large files to .gitignore to avoid committing them to Git.\n",
    "\n",
    "Testing Locally:\n",
    "\n",
    "\t•\tFor local development, ensure your AWS credentials are set up via aws configure or environment variables.\n",
    "\t•\tTest pulling data with dvc pull before pushing changes.\n",
    "\n",
    "Example IAM Policy for GitHub Actions User\n",
    "\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"AllowS3DVCOperations\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:ListBucket\",\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:PutObject\",\n",
    "        \"s3:DeleteObject\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:s3:::your-bucket-name\",\n",
    "        \"arn:aws:s3:::your-bucket-name/*\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\t•\tThis policy allows listing the bucket and performing Get, Put, and Delete operations on objects within the specified S3 bucket.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "By making these changes, you’ve adapted the CI/CD pipeline to work with AWS instead of GCP. Your model is stored in S3, and both DVC and GitHub Actions are configured to access it securely. The Docker image is built and tested in the GitHub Actions environment, pulling the necessary model files from S3.\n",
    "\n",
    "References\n",
    "\n",
    "\t•\tDVC S3 Remote Documentation: DVC Remote Add\n",
    "\t•\tAWS GitHub Actions: aws-actions/configure-aws-credentials\n",
    "\t•\tGitHub Secrets: Encrypted secrets\n",
    "\t•\tDocker GitHub Actions: Docker Build-Push Action\n",
    "\n",
    "Feel free to ask if you need further clarification or assistance with any of the steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understood! If you’d like to verify or print your AWS credentials on your Mac terminal, I can guide you through the steps. However, please be cautious when handling your AWS Secret Access Key, as exposing it can lead to security risks.\n",
    "\n",
    "Viewing AWS Credentials in the Terminal\n",
    "\n",
    "1. Using the AWS CLI to View Configuration\n",
    "\n",
    "If you have the AWS CLI installed and configured, you can use the following command to view your current AWS configuration:\n",
    "\n",
    "aws configure list\n",
    "\n",
    "Example Output:\n",
    "\n",
    "      Name                    Value             Type    Location\n",
    "      ----                    -----             ----    --------\n",
    "   profile                <not set>             None    None\n",
    "access_key     ****************ABCD      config-file    ~/.aws/credentials\n",
    "secret_key     ****************1234      config-file    ~/.aws/credentials\n",
    "    region                us-east-1      config-file    ~/.aws/config\n",
    "\n",
    "\t•\tNote: The access_key and secret_key are partially masked for security.\n",
    "\n",
    "2. Viewing the AWS Credentials File\n",
    "\n",
    "Your AWS credentials are typically stored in the ~/.aws/credentials file. You can view this file using:\n",
    "\n",
    "cat ~/.aws/credentials\n",
    "\n",
    "Example Output:\n",
    "\n",
    "[default]\n",
    "aws_access_key_id = YOUR_ACCESS_KEY_ID\n",
    "aws_secret_access_key = YOUR_SECRET_ACCESS_KEY\n",
    "\n",
    "\t•\tWarning: This will display your actual AWS Access Key ID and Secret Access Key. Be extremely careful not to share or expose this information.\n",
    "\n",
    "3. Viewing Environment Variables\n",
    "\n",
    "If you’ve set your AWS credentials as environment variables, you can print them using:\n",
    "\n",
    "echo $AWS_ACCESS_KEY_ID\n",
    "echo $AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "\t•\tAgain, be cautious when displaying the AWS_SECRET_ACCESS_KEY.\n",
    "\n",
    "Best Practices\n",
    "\n",
    "\t•\tDo Not Expose Secret Keys: Avoid displaying or sharing your AWS Secret Access Key in plaintext. This key grants full access to your AWS resources.\n",
    "\t•\tUse IAM Roles When Possible: If you’re running applications on AWS services like EC2 or ECS, consider using IAM roles instead of hardcoding credentials.\n",
    "\n",
    "Verifying AWS Credentials\n",
    "\n",
    "If you want to test whether your AWS credentials are correctly configured without exposing them, you can perform a simple AWS CLI command:\n",
    "\n",
    "aws sts get-caller-identity\n",
    "\n",
    "Example Output:\n",
    "\n",
    "{\n",
    "    \"UserId\": \"AIDXXXXXXXXXXXXXXX\",\n",
    "    \"Account\": \"123456789012\",\n",
    "    \"Arn\": \"arn:aws:iam::123456789012:user/your-username\"\n",
    "}\n",
    "\n",
    "This command confirms that your AWS CLI is using valid credentials and shows which IAM user or role is being used.\n",
    "\n",
    "Setting AWS Credentials as Environment Variables\n",
    "\n",
    "If you need to set your AWS credentials in your terminal session, you can export them as environment variables:\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=your_access_key_id\n",
    "export AWS_SECRET_ACCESS_KEY=your_secret_access_key\n",
    "export AWS_DEFAULT_REGION=your_region\n",
    "\n",
    "\t•\tReplace your_access_key_id, your_secret_access_key, and your_region with your actual credentials and preferred region.\n",
    "\n",
    "Editing AWS Credentials\n",
    "\n",
    "If you need to edit your AWS credentials, you can use the AWS CLI’s built-in command:\n",
    "\n",
    "aws configure\n",
    "\n",
    "You’ll be prompted to enter your AWS Access Key ID, Secret Access Key, default region, and default output format.\n",
    "\n",
    "Security Reminder\n",
    "\n",
    "\t•\tKeep Credentials Secure: Always ensure that your AWS credentials are kept secure and are not exposed in logs, code repositories, or shared terminals.\n",
    "\t•\tRotate Keys Regularly: It’s good practice to rotate your AWS access keys periodically.\n",
    "\n",
    "Additional Information\n",
    "\n",
    "\t•\tAWS Credentials File Location:\n",
    "\t•\tCredentials: ~/.aws/credentials\n",
    "\t•\tConfigurations: ~/.aws/config\n",
    "\t•\tAWS CLI Version: You can check your AWS CLI version with:\n",
    "\n",
    "aws --version\n",
    "\n",
    "\n",
    "\t•\tInstalling AWS CLI: If you haven’t installed the AWS CLI, you can install it using Homebrew:\n",
    "\n",
    "brew install awscli\n",
    "\n",
    "\n",
    "\n",
    "Need Further Assistance?\n",
    "\n",
    "If you have any more questions or need help with specific tasks related to AWS credentials or configuring your CI/CD pipeline, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Configuring GitHub Actions to Authenticate with AWS (Option A)\n",
    "\n",
    "Since you’ve chosen Option A—pulling the model during the Docker build—you need to ensure that AWS credentials are securely available during the Docker build process in GitHub Actions.\n",
    "\n",
    "Passing AWS credentials during the Docker build requires careful handling to avoid exposing sensitive information. We’ll use Docker BuildKit’s secrets feature to securely pass AWS credentials without including them in the final image layers or build cache.\n",
    "\n",
    "Step 1: Store AWS Credentials as GitHub Secrets\n",
    "\n",
    "Ensure that your AWS credentials are stored as GitHub Secrets:\n",
    "\n",
    "\t•\tAWS_ACCESS_KEY_ID\n",
    "\t•\tAWS_SECRET_ACCESS_KEY\n",
    "\t•\tAWS_DEFAULT_REGION\n",
    "\n",
    "To add these secrets:\n",
    "\n",
    "\t1.\tNavigate to your GitHub repository.\n",
    "\t2.\tGo to Settings > Secrets and variables > Actions > New repository secret.\n",
    "\t3.\tAdd each secret by entering its name and value.\n",
    "\n",
    "Step 2: Modify GitHub Actions Workflow\n",
    "\n",
    "Update your GitHub Actions workflow to:\n",
    "\n",
    "\t•\tEnable Docker BuildKit.\n",
    "\t•\tPass AWS credentials as secrets during the Docker build.\n",
    "\t•\tEnsure that the credentials are not exposed in logs or image layers.\n",
    "\n",
    "Here’s the updated workflow:\n",
    "\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on: [push]\n",
    "\n",
    "env:\n",
    "  DOCKER_BUILDKIT: 1  # Enable Docker BuildKit\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout Code\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "\n",
    "      - name: Build Docker Image\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "        run: |\n",
    "          docker build \\\n",
    "            --secret id=aws_access_key_id,env=AWS_ACCESS_KEY_ID \\\n",
    "            --secret id=aws_secret_access_key,env=AWS_SECRET_ACCESS_KEY \\\n",
    "            --secret id=aws_default_region,env=AWS_DEFAULT_REGION \\\n",
    "            -t inference:latest .\n",
    "\n",
    "      - name: Run Docker Container\n",
    "        run: |\n",
    "          docker run -d -p 8000:8000 --name inference_container inference:latest\n",
    "\n",
    "      - name: Test API Endpoint\n",
    "        run: |\n",
    "          sleep 5  # Wait for the server to start\n",
    "          curl -f http://localhost:8000/health\n",
    "\n",
    "      - name: Clean Up\n",
    "        run: |\n",
    "          docker stop inference_container\n",
    "          docker rm inference_container\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\t•\tEnable Docker BuildKit:\n",
    "\t•\tSet DOCKER_BUILDKIT: 1 in the env section to enable BuildKit features.\n",
    "\t•\tBuild Docker Image:\n",
    "\t•\tUse the --secret flag to pass AWS credentials as secrets.\n",
    "\t•\tThe env section makes the secrets available as environment variables for the build command.\n",
    "\t•\tRun Docker Container and Test API Endpoint:\n",
    "\t•\tStart the container and test your API as before.\n",
    "\n",
    "5. Updating the Dockerfile (Option A)\n",
    "\n",
    "Modify your Dockerfile to:\n",
    "\n",
    "\t•\tUse BuildKit’s secret mounting to access AWS credentials during the build.\n",
    "\t•\tPull the model using DVC during the Docker build without exposing credentials.\n",
    "\n",
    "Updated Dockerfile:\n",
    "\n",
    "# syntax=docker/dockerfile:1.3\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy project files\n",
    "COPY . /app\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements_inference.txt\n",
    "RUN pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "# Initialize DVC\n",
    "RUN dvc init -f --no-scm\n",
    "\n",
    "# Configure the S3 remote\n",
    "RUN dvc remote add -d storage s3://cola-classification/dvc-files\n",
    "\n",
    "# Pull the trained model during build using BuildKit secrets\n",
    "RUN --mount=type=secret,id=aws_access_key_id \\\n",
    "    --mount=type=secret,id=aws_secret_access_key \\\n",
    "    --mount=type=secret,id=aws_default_region \\\n",
    "    export AWS_ACCESS_KEY_ID=$(cat /run/secrets/aws_access_key_id) && \\\n",
    "    export AWS_SECRET_ACCESS_KEY=$(cat /run/secrets/aws_secret_access_key) && \\\n",
    "    export AWS_DEFAULT_REGION=$(cat /run/secrets/aws_default_region) && \\\n",
    "    dvc pull dvcfiles/trained_model.dvc\n",
    "\n",
    "# Expose port and set the command\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\t•\tDockerfile Syntax Directive:\n",
    "\t•\t# syntax=docker/dockerfile:1.3 enables BuildKit features in the Dockerfile.\n",
    "\t•\tMounting Secrets:\n",
    "\t•\tRUN --mount=type=secret,id=aws_access_key_id mounts the secret with ID aws_access_key_id.\n",
    "\t•\tSecrets are accessed from /run/secrets/ directory.\n",
    "\t•\tExporting Environment Variables:\n",
    "\t•\tRead the secrets from the mounted files and export them as environment variables.\n",
    "\t•\tPulling the Model:\n",
    "\t•\tWith AWS credentials set, dvc pull can securely access your S3 remote to retrieve the model.\n",
    "\n",
    "Important Notes:\n",
    "\n",
    "\t•\tSecurity:\n",
    "\t•\tCredentials are not stored in image layers or build cache.\n",
    "\t•\tThe secrets are only available during the build step where they’re needed.\n",
    "\t•\tBuildKit Requirement:\n",
    "\t•\tBoth the Dockerfile and the build command in GitHub Actions must be configured to use BuildKit.\n",
    "\n",
    "6. Additional Considerations\n",
    "\n",
    "Testing Locally with BuildKit\n",
    "\n",
    "If you want to test the Docker build locally:\n",
    "\n",
    "\t1.\tEnable BuildKit:\n",
    "\n",
    "export DOCKER_BUILDKIT=1\n",
    "\n",
    "\n",
    "\t2.\tBuild the image with secrets:\n",
    "\n",
    "docker build \\\n",
    "  --secret id=aws_access_key_id,env=AWS_ACCESS_KEY_ID \\\n",
    "  --secret id=aws_secret_access_key,env=AWS_SECRET_ACCESS_KEY \\\n",
    "  --secret id=aws_default_region,env=AWS_DEFAULT_REGION \\\n",
    "  -t inference:latest .\n",
    "\n",
    "\n",
    "\t3.\tEnsure that AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_DEFAULT_REGION are set in your local environment.\n",
    "\n",
    "Ensuring No Credential Leakage\n",
    "\n",
    "\t•\tAvoid using docker build options like --progress=plain which might output sensitive information.\n",
    "\t•\tEnsure that any logging in the Docker build process does not print environment variables or secrets.\n",
    "\n",
    "7. Complete GitHub Actions Workflow (Option A)\n",
    "\n",
    "Updated Workflow File (.github/workflows/ci-cd.yaml):\n",
    "\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on: [push]\n",
    "\n",
    "env:\n",
    "  DOCKER_BUILDKIT: 1  # Enable Docker BuildKit\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "      - name: Checkout Code\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v2\n",
    "\n",
    "      - name: Log in to Amazon ECR (Optional)\n",
    "        uses: aws-actions/amazon-ecr-login@v1\n",
    "        with:\n",
    "          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "\n",
    "      - name: Build Docker Image\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "        run: |\n",
    "          docker build \\\n",
    "            --secret id=aws_access_key_id,env=AWS_ACCESS_KEY_ID \\\n",
    "            --secret id=aws_secret_access_key,env=AWS_SECRET_ACCESS_KEY \\\n",
    "            --secret id=aws_default_region,env=AWS_DEFAULT_REGION \\\n",
    "            -t inference:latest .\n",
    "\n",
    "      - name: Run Docker Container\n",
    "        run: |\n",
    "          docker run -d -p 8000:8000 --name inference_container inference:latest\n",
    "\n",
    "      - name: Test API Endpoint\n",
    "        run: |\n",
    "          sleep 5  # Wait for the server to start\n",
    "          curl -f http://localhost:8000/health\n",
    "\n",
    "      - name: Clean Up\n",
    "        run: |\n",
    "          docker stop inference_container\n",
    "          docker rm inference_container\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\t•\tSet up Docker Buildx:\n",
    "\t•\tNecessary for advanced build features like BuildKit secrets.\n",
    "\t•\tOptional ECR Login:\n",
    "\t•\tIf you plan to push your Docker image to Amazon ECR, include the aws-actions/amazon-ecr-login@v1 action.\n",
    "\t•\tBuild Docker Image:\n",
    "\t•\tPass AWS credentials as secrets using --secret flags.\n",
    "\t•\tEnvironment variables are set from GitHub Secrets.\n",
    "\t•\tRun and Test:\n",
    "\t•\tContainer is run and the API endpoint is tested as before.\n",
    "\n",
    "8. Security Best Practices\n",
    "\n",
    "\t•\tUse Least Privilege Principle:\n",
    "\t•\tThe IAM user or role used for AWS credentials should have minimal permissions required to access the S3 bucket.\n",
    "\t•\tRotate Credentials Regularly:\n",
    "\t•\tUpdate AWS credentials periodically and update GitHub Secrets accordingly.\n",
    "\t•\tMonitor Logs:\n",
    "\t•\tEnsure logs in GitHub Actions and Docker build do not contain sensitive information.\n",
    "\n",
    "9. Final Summary\n",
    "\n",
    "By choosing Option A and following the steps:\n",
    "\n",
    "\t•\tConfigured GitHub Actions to securely authenticate with AWS during the Docker build using BuildKit secrets.\n",
    "\t•\tModified the Dockerfile to use BuildKit’s secret mounting to access AWS credentials without exposing them.\n",
    "\t•\tEnsured Security by preventing credentials from being stored in image layers or logs.\n",
    "\t•\tUpdated the GitHub Actions workflow to build, run, and test your Docker image in a secure and automated manner.\n",
    "\n",
    "References\n",
    "\n",
    "\t•\tDocker BuildKit Secrets:\n",
    "\t•\tDocker Documentation - Build secrets and SSH forwarding\n",
    "\t•\tGitHub Actions Docker Build Example:\n",
    "\t•\tUsing secrets in builds\n",
    "\t•\tAWS GitHub Actions:\n",
    "\t•\taws-actions/configure-aws-credentials\n",
    "\t•\taws-actions/amazon-ecr-login\n",
    "\t•\tDVC S3 Remote Documentation:\n",
    "\t•\tDVC Remote Add\n",
    "\n",
    "Feel Free to Ask Questions\n",
    "\n",
    "If you need further clarification or assistance with any of the steps, don’t hesitate to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Since you’re interested in Option B, where the model is pulled at runtime when the container starts, I’ll provide you with the full Dockerfile and ci-cd.yaml (GitHub Actions workflow) files tailored for this approach.\n",
    "\n",
    "Option B Overview\n",
    "\n",
    "\t•\tDockerfile: The Docker image does not include the model; instead, the model is pulled using DVC when the container starts.\n",
    "\t•\tGitHub Actions Workflow: AWS credentials are provided to the container at runtime, not during the build process.\n",
    "\t•\tSecurity Considerations: Since the AWS credentials are provided at runtime, they are not part of the Docker image layers, enhancing security.\n",
    "\n",
    "Dockerfile for Option B\n",
    "\n",
    "Here’s the complete Dockerfile for Option B:\n",
    "\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy project files\n",
    "COPY . /app\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements_inference.txt\n",
    "RUN pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "# Initialize DVC\n",
    "RUN dvc init -f --no-scm\n",
    "\n",
    "# Configure the S3 remote\n",
    "RUN dvc remote add -d storage s3://cola-classification/dvc-files\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Set the command to run when the container starts\n",
    "CMD [\"sh\", \"-c\", \"dvc pull dvcfiles/trained_model.dvc && uvicorn app:app --host 0.0.0.0 --port 8000\"]\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\t•\tNo Model Pull During Build:\n",
    "\t•\tThe dvc pull command is not run during the build process.\n",
    "\t•\tThis avoids the need to provide AWS credentials during the build.\n",
    "\t•\tCMD Instruction:\n",
    "\t•\tThe CMD runs a shell command that first pulls the model using DVC and then starts the application using Uvicorn.\n",
    "\t•\tThe AWS credentials must be available as environment variables when the container runs.\n",
    "\t•\tSecurity:\n",
    "\t•\tSince the model is pulled at runtime, AWS credentials are not included in the image layers.\n",
    "\n",
    "GitHub Actions Workflow (ci-cd.yaml) for Option B\n",
    "\n",
    "Here’s the complete ci-cd.yaml file for Option B:\n",
    "\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    steps:\n",
    "      - name: Checkout Code\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "\n",
    "      - name: Install Dependencies\n",
    "        run: |\n",
    "          pip install --no-cache-dir -r requirements_inference.txt\n",
    "          pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "      - name: Configure AWS Credentials for DVC\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "        run: |\n",
    "          dvc remote modify storage --local \\\n",
    "            access_key_id $AWS_ACCESS_KEY_ID\n",
    "          dvc remote modify storage --local \\\n",
    "            secret_access_key $AWS_SECRET_ACCESS_KEY\n",
    "          dvc remote modify storage --local \\\n",
    "            region $AWS_DEFAULT_REGION\n",
    "\n",
    "      - name: Pull DVC Data\n",
    "        run: |\n",
    "          dvc pull dvcfiles/trained_model.dvc\n",
    "\n",
    "      - name: Build Docker Image\n",
    "        run: |\n",
    "          docker build -t inference:latest .\n",
    "\n",
    "      - name: Run Docker Container\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "        run: |\n",
    "          docker run -d -p 8000:8000 \\\n",
    "            -e AWS_ACCESS_KEY_ID \\\n",
    "            -e AWS_SECRET_ACCESS_KEY \\\n",
    "            -e AWS_DEFAULT_REGION \\\n",
    "            --name inference_container inference:latest\n",
    "\n",
    "      - name: Test API Endpoint\n",
    "        run: |\n",
    "          sleep 5  # Wait for the server to start\n",
    "          curl -f http://localhost:8000/health\n",
    "\n",
    "      - name: Clean Up\n",
    "        run: |\n",
    "          docker stop inference_container\n",
    "          docker rm inference_container\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\t•\tCheckout Code:\n",
    "\t•\tRetrieves the repository content.\n",
    "\t•\tSet up Python and Install Dependencies:\n",
    "\t•\tPrepares the environment to run DVC commands.\n",
    "\t•\tConfigure AWS Credentials for DVC:\n",
    "\t•\tSets up DVC to use AWS credentials for pulling data.\n",
    "\t•\tUses --local flag to avoid saving credentials in Git-tracked files.\n",
    "\t•\tPull DVC Data:\n",
    "\t•\tPulls the model files into the workspace. (Optional, see note below.)\n",
    "\t•\tBuild Docker Image:\n",
    "\t•\tBuilds the Docker image without including the model.\n",
    "\t•\tRun Docker Container:\n",
    "\t•\tStarts the container, passing AWS credentials as environment variables.\n",
    "\t•\tThe container will pull the model at runtime.\n",
    "\t•\tTest API Endpoint:\n",
    "\t•\tWaits for the application to start and checks the health endpoint.\n",
    "\t•\tClean Up:\n",
    "\t•\tStops and removes the container.\n",
    "\n",
    "Notes:\n",
    "\n",
    "\t•\tOptional DVC Pull in Workflow:\n",
    "\t•\tThe Pull DVC Data step is optional since the model will be pulled inside the container at runtime.\n",
    "\t•\tHowever, pulling the model during the workflow can be useful to verify that the DVC configuration and AWS credentials are correct.\n",
    "\t•\tPassing AWS Credentials:\n",
    "\t•\tAWS credentials are provided to the container via environment variables.\n",
    "\t•\tEnsure that the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_DEFAULT_REGION secrets are set in your GitHub repository.\n",
    "\n",
    "Security Considerations\n",
    "\n",
    "\t•\tAvoid Exposing Credentials:\n",
    "\t•\tDo not print or log AWS credentials.\n",
    "\t•\tUse GitHub Secrets to store sensitive information securely.\n",
    "\t•\tLeast Privilege Principle:\n",
    "\t•\tThe AWS IAM user should have minimal permissions required to access the S3 bucket.\n",
    "\t•\tAvoid granting unnecessary permissions.\n",
    "\t•\tAvoid Committing Credentials:\n",
    "\t•\tThe --local flag in dvc remote modify ensures that credentials are stored in .dvc/config.local, which is ignored by Git.\n",
    "\n",
    "Detailed Steps and Commands\n",
    "\n",
    "1. Dockerfile Breakdown\n",
    "\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . /app\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements_inference.txt\n",
    "RUN pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "RUN dvc init -f --no-scm\n",
    "RUN dvc remote add -d storage s3://cola-classification/dvc-files\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"sh\", \"-c\", \"dvc pull dvcfiles/trained_model.dvc && uvicorn app:app --host 0.0.0.0 --port 8000\"]\n",
    "\n",
    "\t•\tFROM python:3.9-slim:\n",
    "\t•\tUses a slim Python 3.9 base image to keep the image size small.\n",
    "\t•\tWORKDIR /app and COPY . /app:\n",
    "\t•\tSets the working directory and copies all files from your project into the container.\n",
    "\t•\tInstall Dependencies:\n",
    "\t•\tInstalls application dependencies and DVC with S3 support.\n",
    "\t•\tDVC Initialization and Remote Configuration:\n",
    "\t•\tInitializes DVC and adds the S3 remote named storage.\n",
    "\t•\tCMD Instruction:\n",
    "\t•\tWhen the container starts, it runs:\n",
    "\t•\tdvc pull dvcfiles/trained_model.dvc to pull the model from S3.\n",
    "\t•\tStarts the application using Uvicorn.\n",
    "\n",
    "2. GitHub Actions Workflow Breakdown\n",
    "\n",
    "Checkout Code and Set up Python\n",
    "\n",
    "- name: Checkout Code\n",
    "  uses: actions/checkout@v3\n",
    "\n",
    "- name: Set up Python\n",
    "  uses: actions/setup-python@v4\n",
    "  with:\n",
    "    python-version: '3.9'\n",
    "\n",
    "\t•\tChecks out the repository and sets up Python 3.9.\n",
    "\n",
    "Install Dependencies\n",
    "\n",
    "- name: Install Dependencies\n",
    "  run: |\n",
    "    pip install --no-cache-dir -r requirements_inference.txt\n",
    "    pip install --no-cache-dir \"dvc[s3]\"\n",
    "\n",
    "\t•\tInstalls required Python packages and DVC with S3 support.\n",
    "\n",
    "Configure AWS Credentials for DVC\n",
    "\n",
    "- name: Configure AWS Credentials for DVC\n",
    "  env:\n",
    "    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "    AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "  run: |\n",
    "    dvc remote modify storage --local \\\n",
    "      access_key_id $AWS_ACCESS_KEY_ID\n",
    "    dvc remote modify storage --local \\\n",
    "      secret_access_key $AWS_SECRET_ACCESS_KEY\n",
    "    dvc remote modify storage --local \\\n",
    "      region $AWS_DEFAULT_REGION\n",
    "\n",
    "\t•\tModifies the DVC remote configuration to include AWS credentials locally.\n",
    "\t•\tUses --local to ensure credentials are not saved in Git-tracked files.\n",
    "\n",
    "Pull DVC Data (Optional)\n",
    "\n",
    "- name: Pull DVC Data\n",
    "  run: |\n",
    "    dvc pull dvcfiles/trained_model.dvc\n",
    "\n",
    "\t•\tPulls the model files from S3 to the workspace.\n",
    "\t•\tThis step verifies that the DVC setup and credentials are correct.\n",
    "\n",
    "Build Docker Image\n",
    "\n",
    "- name: Build Docker Image\n",
    "  run: |\n",
    "    docker build -t inference:latest .\n",
    "\n",
    "\t•\tBuilds the Docker image using the Dockerfile in the current directory.\n",
    "\n",
    "Run Docker Container\n",
    "\n",
    "- name: Run Docker Container\n",
    "  env:\n",
    "    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "    AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}\n",
    "  run: |\n",
    "    docker run -d -p 8000:8000 \\\n",
    "      -e AWS_ACCESS_KEY_ID \\\n",
    "      -e AWS_SECRET_ACCESS_KEY \\\n",
    "      -e AWS_DEFAULT_REGION \\\n",
    "      --name inference_container inference:latest\n",
    "\n",
    "\t•\tRuns the container, passing AWS credentials as environment variables.\n",
    "\t•\tThe container uses these credentials to pull the model at runtime.\n",
    "\n",
    "Test API Endpoint\n",
    "\n",
    "- name: Test API Endpoint\n",
    "  run: |\n",
    "    sleep 5  # Wait for the server to start\n",
    "    curl -f http://localhost:8000/health\n",
    "\n",
    "\t•\tWaits for the application to start and tests the health endpoint.\n",
    "\n",
    "Clean Up\n",
    "\n",
    "- name: Clean Up\n",
    "  run: |\n",
    "    docker stop inference_container\n",
    "    docker rm inference_container\n",
    "\n",
    "\t•\tStops and removes the Docker container.\n",
    "\n",
    "Setting Up AWS Credentials in GitHub Secrets\n",
    "\n",
    "Ensure that you have added the following secrets in your GitHub repository:\n",
    "\n",
    "\t•\tAWS_ACCESS_KEY_ID\n",
    "\t•\tAWS_SECRET_ACCESS_KEY\n",
    "\t•\tAWS_DEFAULT_REGION\n",
    "\n",
    "Steps:\n",
    "\n",
    "\t1.\tGo to your repository on GitHub.\n",
    "\t2.\tClick on Settings.\n",
    "\t3.\tIn the left sidebar, click on Secrets and variables and then Actions.\n",
    "\t4.\tClick on New repository secret.\n",
    "\t5.\tAdd each secret by providing the name and value.\n",
    "\n",
    "Local Testing Tips\n",
    "\n",
    "\t•\tTest the Docker Image Locally:\n",
    "\t•\tBuild the Docker image:\n",
    "\n",
    "docker build -t inference:latest .\n",
    "\n",
    "\n",
    "\t•\tRun the Docker container with AWS credentials:\n",
    "\n",
    "docker run -d -p 8000:8000 \\\n",
    "  -e AWS_ACCESS_KEY_ID=your_access_key_id \\\n",
    "  -e AWS_SECRET_ACCESS_KEY=your_secret_access_key \\\n",
    "  -e AWS_DEFAULT_REGION=your_region \\\n",
    "  --name inference_container inference:latest\n",
    "\n",
    "\n",
    "\t•\tReplace your_access_key_id, your_secret_access_key, and your_region with your actual AWS credentials and region.\n",
    "\n",
    "\t•\tTest the Application:\n",
    "\t•\tWait a few seconds for the application to start.\n",
    "\t•\tRun:\n",
    "\n",
    "curl -f http://localhost:8000/health\n",
    "\n",
    "\n",
    "\t•\tYou should receive a successful response if everything is set up correctly.\n",
    "\n",
    "Advantages of Option B\n",
    "\n",
    "\t•\tEnhanced Security:\n",
    "\t•\tAWS credentials are only used at runtime, reducing the risk of exposure.\n",
    "\t•\tThe Docker image does not contain any sensitive information.\n",
    "\t•\tFlexibility:\n",
    "\t•\tThe model can be updated in the S3 bucket, and the container will pull the latest version each time it starts.\n",
    "\t•\tSmaller Image Size:\n",
    "\t•\tThe Docker image is smaller because it does not include the model files.\n",
    "\n",
    "Possible Modifications\n",
    "\n",
    "\t•\tCaching the Model:\n",
    "\t•\tIf pulling the model each time the container starts is time-consuming, consider implementing a caching mechanism within the container.\n",
    "\t•\tError Handling:\n",
    "\t•\tModify the CMD in the Dockerfile to handle errors during dvc pull, such as retrying or exiting gracefully.\n",
    "\t•\tEnvironment Variables for DVC Configuration:\n",
    "\t•\tInstead of modifying DVC config files, you can set AWS credentials using environment variables inside the container.\n",
    "\n",
    "ENV AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n",
    "ENV AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n",
    "ENV AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n",
    "\n",
    "\t•\tHowever, this approach would require passing the credentials as build arguments, which reintroduces the need to handle credentials during the build process.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "By using Option B, you’ve set up a CI/CD pipeline that:\n",
    "\n",
    "\t•\tBuilds a Docker image without including the model or sensitive credentials.\n",
    "\t•\tRuns the container, pulling the model at runtime using AWS credentials provided via environment variables.\n",
    "\t•\tTests the application, ensuring that it starts correctly and the model is accessible.\n",
    "\n",
    "This approach enhances security and flexibility, making it suitable for scenarios where the model may be updated frequently or where including credentials in the image build is undesirable.\n",
    "\n",
    "Let Me Know If You Need Further Assistance\n",
    "\n",
    "If you have any questions or need additional help customizing the Dockerfile or workflow, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing between Option A (pulling the model during the Docker build) and Option B (pulling the model at runtime when the container starts) depends on your specific requirements, priorities, and constraints. I’ll help you evaluate both options by comparing their advantages and disadvantages, so you can decide which one best suits your needs.\n",
    "\n",
    "Option A: Pulling the Model During Docker Build\n",
    "\n",
    "Overview\n",
    "\n",
    "\t•\tThe model is pulled from S3 using DVC during the Docker image build process.\n",
    "\t•\tAWS credentials are provided securely during the build using Docker BuildKit secrets.\n",
    "\t•\tThe final Docker image includes the model.\n",
    "\n",
    "Advantages\n",
    "\n",
    "\t1.\tPerformance Efficiency\n",
    "\t•\tFaster Startup Time: Since the model is already included in the image, the container starts quickly without needing to download the model at runtime.\n",
    "\t•\tConsistent Environment: Every instance of the container uses the exact same model version baked into the image.\n",
    "\t2.\tSimplified Deployment\n",
    "\t•\tNo Need for AWS Credentials at Runtime: The container does not require AWS credentials when it runs, reducing runtime dependencies.\n",
    "\t•\tEasier Scaling: Spinning up multiple containers is straightforward, as they all have the model included.\n",
    "\t3.\tImmutable Artifacts\n",
    "\t•\tVersion Control: The Docker image acts as a snapshot, ensuring that the code and the model are consistent.\n",
    "\t•\tReproducibility: The environment is fully encapsulated, aiding in debugging and rollbacks.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "\t1.\tSecurity Considerations\n",
    "\t•\tCredentials During Build: AWS credentials are used during the build process. Although BuildKit helps secure them, there is a potential risk if not handled properly.\n",
    "\t•\tLarger Image Size: Including the model increases the size of the Docker image, which can affect storage costs and deployment times.\n",
    "\t2.\tModel Updates\n",
    "\t•\tLess Flexibility: Updating the model requires rebuilding and redeploying the Docker image.\n",
    "\t•\tSlower CI/CD Pipeline: The build process takes longer because it includes downloading the model.\n",
    "\t3.\tBuild Complexity\n",
    "\t•\tAdvanced Configuration: Requires enabling Docker BuildKit and properly configuring secrets, which may add complexity to your CI/CD setup.\n",
    "\n",
    "Option B: Pulling the Model at Runtime\n",
    "\n",
    "Overview\n",
    "\n",
    "\t•\tThe Docker image does not include the model.\n",
    "\t•\tThe model is pulled from S3 using DVC when the container starts.\n",
    "\t•\tAWS credentials are required at runtime.\n",
    "\n",
    "Advantages\n",
    "\n",
    "\t1.\tSecurity\n",
    "\t•\tNo Credentials During Build: AWS credentials are not used during the image build, reducing the risk of exposure.\n",
    "\t•\tSmaller Attack Surface: The Docker image does not contain sensitive information or the model.\n",
    "\t2.\tFlexibility\n",
    "\t•\tDynamic Model Updates: Updating the model in S3 allows containers to use the latest model without rebuilding the image.\n",
    "\t•\tFaster Build Times: The Docker image builds quickly since it doesn’t include the model.\n",
    "\t3.\tSmaller Image Size\n",
    "\t•\tEfficient Storage and Transfer: Smaller images reduce storage costs and speed up deployments.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "\t1.\tRuntime Dependencies\n",
    "\t•\tAWS Credentials Needed at Runtime: You must securely provide AWS credentials to the container, which can be a security concern if not managed properly.\n",
    "\t•\tPotential Delays on Startup: Pulling the model at runtime can increase the time it takes for the container to become ready.\n",
    "\t2.\tOperational Complexity\n",
    "\t•\tHandling Failures: If the model fails to download at runtime, the container may not start correctly, requiring robust error handling.\n",
    "\t•\tScaling Challenges: Starting many containers simultaneously could strain bandwidth or S3 limits.\n",
    "\t3.\tInconsistent Environments\n",
    "\t•\tModel Version Drift: If the model in S3 is updated, containers might run different model versions if not managed carefully.\n",
    "\t•\tCache Management: Without proper caching, each container start could lead to redundant downloads.\n",
    "\n",
    "Factors to Consider\n",
    "\n",
    "1. Frequency of Model Updates\n",
    "\n",
    "\t•\tOption A: Better if the model changes infrequently, as rebuilding the image each time can be cumbersome.\n",
    "\t•\tOption B: Suited for environments where the model is updated regularly, allowing containers to fetch the latest model without rebuilding.\n",
    "\n",
    "2. Deployment Environment\n",
    "\n",
    "\t•\tOption A: Ideal for environments with strict runtime security policies that restrict access to AWS credentials.\n",
    "\t•\tOption B: Fits well in dynamic environments like Kubernetes, where sidecar containers or init containers can manage dependencies.\n",
    "\n",
    "3. Security Requirements\n",
    "\n",
    "\t•\tOption A: Reduces the need for runtime credentials, minimizing the risk of credential leakage during operation.\n",
    "\t•\tOption B: Requires secure handling of AWS credentials at runtime, potentially increasing security risks if not managed properly.\n",
    "\n",
    "4. Startup Performance\n",
    "\n",
    "\t•\tOption A: Containers start faster since the model is already available.\n",
    "\t•\tOption B: Containers have additional startup time due to model download, which may affect auto-scaling responsiveness.\n",
    "\n",
    "5. Image Size and Distribution\n",
    "\n",
    "\t•\tOption A: Larger images may consume more bandwidth and storage, affecting deployment speed and cost.\n",
    "\t•\tOption B: Smaller images are quicker to distribute and deploy.\n",
    "\n",
    "6. CI/CD Pipeline Complexity\n",
    "\n",
    "\t•\tOption A: More complex to set up due to BuildKit and secure handling of credentials during build.\n",
    "\t•\tOption B: Simpler build process but may require additional runtime configuration and error handling.\n",
    "\n",
    "Recommendations\n",
    "\n",
    "Option A is Best When:\n",
    "\n",
    "\t•\tSecurity Policies Favor No Runtime Credentials: If it’s critical to avoid having AWS credentials in the runtime environment.\n",
    "\t•\tConsistent Environments Are Required: When you need to ensure that every container runs with the exact same model version.\n",
    "\t•\tStartup Performance Is Crucial: If rapid container startup times are necessary for your application.\n",
    "\n",
    "Option B is Best When:\n",
    "\n",
    "\t•\tModels Are Updated Frequently: If you need to deploy new models often without rebuilding the Docker image.\n",
    "\t•\tFlexibility Is Important: When you want the ability to update the model independently of the application code.\n",
    "\t•\tMinimizing Image Size Matters: If storage and network bandwidth are significant concerns.\n",
    "\n",
    "My Suggestion\n",
    "\n",
    "Based on the considerations:\n",
    "\n",
    "\t•\tIf your model updates are infrequent, and security concerns around runtime credentials are high, Option A might be the better choice.\n",
    "\t•\tIf you expect to update your model regularly, or need flexibility to change models without rebuilding images, Option B would be more appropriate.\n",
    "\n",
    "Given that you initially wanted to avoid embedding AWS credentials in the Docker build process (which requires careful handling in Option A), and considering the security implications, Option B might be more suitable, especially if you have robust security practices for handling runtime credentials.\n",
    "\n",
    "Additional Considerations\n",
    "\n",
    "\t•\tHybrid Approach:\n",
    "\t•\tCaching: Implement caching mechanisms in Option B to mitigate startup delays.\n",
    "\t•\tCredential Management: Use services like AWS Secrets Manager or IAM Roles for Tasks (in ECS) or IRSA (IAM Roles for Service Accounts in EKS) to securely provide credentials at runtime without hardcoding them.\n",
    "\t•\tAutomation Tools:\n",
    "\t•\tUse infrastructure as code tools like Terraform to manage AWS resources and permissions.\n",
    "\t•\tImplement monitoring and alerting to catch any issues with model downloads or container startups.\n",
    "\t•\tCompliance Requirements:\n",
    "\t•\tEnsure that your chosen option complies with any industry regulations or organizational policies.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Both options have their merits, and the “best” option depends on your specific use case, priorities, and constraints. Consider the factors outlined above in the context of your project to make an informed decision.\n",
    "\n",
    "If you need:\n",
    "\n",
    "\t•\tSecurity and Consistency: Lean towards Option A.\n",
    "\t•\tFlexibility and Efficiency: Lean towards Option B.\n",
    "\n",
    "Next Steps\n",
    "\n",
    "\t•\tEvaluate Your Requirements:\n",
    "\t•\tList out your priorities regarding security, performance, flexibility, and operational complexity.\n",
    "\t•\tTest Both Options:\n",
    "\t•\tIf feasible, prototype both options in a controlled environment to assess their impacts.\n",
    "\t•\tPlan for Implementation:\n",
    "\t•\tWhichever option you choose, ensure you have the necessary infrastructure and practices in place to support it.\n",
    "\n",
    "Feel Free to Ask More Questions\n",
    "\n",
    "If you have further questions or need assistance with implementing either option, or if there’s specific information about your use case you’d like to share for a more tailored recommendation, please let me know!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
