{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'MIGraphXExecutionProvider', 'ROCMExecutionProvider', 'OpenVINOExecutionProvider', 'DnnlExecutionProvider', 'TvmExecutionProvider', 'VitisAIExecutionProvider', 'QNNExecutionProvider', 'NnapiExecutionProvider', 'VSINPUExecutionProvider', 'JsExecutionProvider', 'CoreMLExecutionProvider', 'ArmNNExecutionProvider', 'ACLExecutionProvider', 'DmlExecutionProvider', 'RknpuExecutionProvider', 'WebNNExecutionProvider', 'XnnpackExecutionProvider', 'CANNExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime import  get_all_providers\n",
    "print(get_all_providers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4abe50d6ab45b8a07c1004bd3b3e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/251k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0f29875a5e49bb92abc8e620f381c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/37.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d3cc1de0984e1887e4ac5cfbd3091f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/37.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4977fd8ef0864370a8516a03372a8437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b6230b36f340dd9e432b8a2f94330c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7164541db8b4480cb419d84b6ea7a3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cola_dataset = load_dataset('glue', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8551, 1043, 1063)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = cola_dataset['train']\n",
    "val_dataset = cola_dataset['validation']\n",
    "test_dataset = cola_dataset['test']\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The sailors rode the breeze clear of the rocks.',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Bill whistled past the house.', 'label': -1, 'idx': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['unacceptable', 'acceptable'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c5e97c40f246268640a27578932991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': [\"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       "  \"One more pseudo generalization and I'm giving up.\",\n",
       "  \"One more pseudo generalization or I'm giving up.\",\n",
       "  'The more we study verbs, the crazier they get.',\n",
       "  'Day by day the facts are getting murkier.'],\n",
       " 'label': [1, 1, 1, 1, 1],\n",
       " 'idx': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.filter(lambda example: example['label'] == train_dataset.features['label'].str2int('acceptable'))[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9bd95830ef4f6b867dd39e9bbb0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': ['They drank the pub.',\n",
       "  'The professor talked us.',\n",
       "  'We yelled ourselves.',\n",
       "  'We yelled Harry hoarse.',\n",
       "  'Harry coughed himself.'],\n",
       " 'label': [0, 0, 0, 0, 0],\n",
       " 'idx': [18, 20, 22, 23, 25]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.filter(lambda example: example['label'] == train_dataset.features['label'].str2int('unacceptable'))[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1812481ffc3e422287d3d11abb00dce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/382 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20677c1faf4d4ef5ae289e9bfd25c87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiansegnou/miniconda3/envs/torch-mps/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google/bert_uncased_L-2_H-128_A-2', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the configuration of a `BertTokenizerFast`, which is part of the Hugging Face Transformers library.\n",
    "\n",
    "### 1. **BertTokenizerFast**:\n",
    "This is a tokenizer specifically designed for BERT models. It uses a \"fast\" implementation that is highly optimized for speed (based on the `tokenizers` library by Hugging Face). \n",
    "\n",
    "Here’s a breakdown of its components:\n",
    "\n",
    "- **name_or_path**: \n",
    "   - `'google/bert_uncased_L-2_H-128_A-2'` refers to the name or path of the BERT model for which this tokenizer is designed. This specific model name suggests it is a variant of BERT that is uncased (doesn't differentiate between uppercase and lowercase characters), with only **2 layers (L-2)**, **128 hidden units (H-128)**, and **2 attention heads (A-2)**. This is a smaller BERT model, typically used for fast, lightweight tasks.\n",
    "   \n",
    "- **vocab_size=30522**:\n",
    "   - This refers to the size of the vocabulary used by the tokenizer. In this case, the vocabulary has 30,522 unique tokens (words or subword units).\n",
    "\n",
    "- **model_max_length=1000000000000000019884624838656**:\n",
    "   - This number seems unusually large. Typically, `model_max_length` specifies the maximum length of tokens the model can handle (for BERT, it's often 512). This value could be a placeholder or could indicate no predefined limit is set.\n",
    "\n",
    "- **is_fast=True**:\n",
    "   - This indicates that the fast version of the tokenizer is used, optimized for speed.\n",
    "\n",
    "- **padding_side='right'** and **truncation_side='right'**:\n",
    "   - These specify that padding and truncation should occur on the right side of the input sequence. This means if the input sequence is shorter than expected, padding tokens will be added to the right. If it’s longer than allowed, it will be truncated from the right side.\n",
    "\n",
    "- **special_tokens**:\n",
    "   - This dictionary defines the special tokens used by BERT. These tokens serve specific purposes in the model:\n",
    "     - `'unk_token': '[UNK]'`: Represents unknown tokens (words not in the tokenizer’s vocabulary).\n",
    "     - `'sep_token': '[SEP]'`: Used to separate segments in BERT (especially in sentence pair tasks).\n",
    "     - `'pad_token': '[PAD]'`: Used for padding short sequences.\n",
    "     - `'cls_token': '[CLS]'`: Added at the beginning of every input sequence for classification tasks.\n",
    "     - `'mask_token': '[MASK]'`: Used in masked language modeling tasks, where some tokens are masked and the model tries to predict them.\n",
    "\n",
    "- **clean_up_tokenization_spaces=True**:\n",
    "   - This is a flag that indicates the tokenizer should clean up extra spaces in the tokenized output. It ensures there aren’t unnecessary spaces between tokens, which can be useful for readability when decoding the tokens back into text.\n",
    "\n",
    "### 2. **added_tokens_decoder**:\n",
    "This part defines how specific tokens are added to the tokenizer's vocabulary, and their characteristics:\n",
    "\n",
    "- **AddedToken**: This is a special token added to the tokenizer’s vocabulary. The properties of each token define how it behaves:\n",
    "   - **\"[PAD]\"** (token ID 0): Represents padding.\n",
    "   - **\"[UNK]\"** (token ID 100): Represents unknown tokens.\n",
    "   - **\"[CLS]\"** (token ID 101): Used for classification.\n",
    "   - **\"[SEP]\"** (token ID 102): Used for separating sentences.\n",
    "   - **\"[MASK]\"** (token ID 103): Used for masked language modeling tasks.\n",
    "\n",
    "Each of these tokens has additional attributes:\n",
    "- **rstrip=False**, **lstrip=False**: These indicate that the token should not strip spaces to the right or left when added to the tokenized sequence.\n",
    "- **single_word=False**: This means that the token is not constrained to a single word but can appear within larger sequences.\n",
    "- **normalized=False**: Indicates that the token should not undergo additional normalization (such as lowercasing or other transformations).\n",
    "- **special=True**: This flag marks the token as special, so it has a unique role in processing (e.g., padding, masking, etc.).\n",
    "\n",
    "### Summary:\n",
    "In essence, this tokenizer is designed to work with a small, uncased version of BERT (2 layers, 128 hidden units), optimized for speed, with a vocabulary size of 30,522. It defines how special tokens like `[PAD]`, `[CLS]`, `[SEP]`, etc., should be handled and includes additional configurations for padding, truncation, and token cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our friends won't buy this analysis, let alone the next one we propose.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset[0]['sentence'])\n",
    "tokenizer(train_dataset[0]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process of tokenizing a sentence using a BERT tokenizer and explains how the tokenized input is structured when using the Hugging Face `BertTokenizerFast`. \n",
    "\n",
    "### 1. **`print(train_dataset[0]['sentence'])`**:\n",
    "   This line prints the first sentence from the training dataset. In your example, the sentence is:\n",
    "\n",
    "   ```\n",
    "   Our friends won't buy this analysis, let alone the next one we propose.\n",
    "   ```\n",
    "\n",
    "### 2. **`tokenizer(train_dataset[0]['sentence'])`**:\n",
    "   This line tokenizes the same sentence using the BERT tokenizer, which outputs a dictionary containing the following fields:\n",
    "   \n",
    "   - **`input_ids`**: This is the list of token IDs corresponding to each word or subword in the sentence. BERT uses a predefined vocabulary to map words and subwords to numeric values. The tokenizer breaks the sentence down into tokens, converts them into IDs, and adds special tokens like `[CLS]` and `[SEP]`.\n",
    "   \n",
    "   Here is the mapping for each token:\n",
    "\n",
    "   | Token | Input ID |\n",
    "   |-------|----------|\n",
    "   | [CLS] | 101      |\n",
    "   | Our   | 2256     |\n",
    "   | friends | 2814    |\n",
    "   | won | 2180       |\n",
    "   | 't | 1005        |\n",
    "   | buy | 4965       |\n",
    "   | this | 2023      |\n",
    "   | analysis | 4106  |\n",
    "   | , | 1010         |\n",
    "   | let | 2292       |\n",
    "   | alone | 2894     |\n",
    "   | the | 1996       |\n",
    "   | next | 2279      |\n",
    "   | one | 2028       |\n",
    "   | we | 2057        |\n",
    "   | propose | 16599  |\n",
    "   | . | 1012         |\n",
    "   | [SEP] | 102      |\n",
    "\n",
    "   - **Special Tokens**: `[CLS]` (ID: 101) is added at the beginning of the sentence and `[SEP]` (ID: 102) is added at the end. These are standard tokens for BERT models:\n",
    "     - `[CLS]`: BERT uses this token at the beginning for classification tasks or sentence understanding.\n",
    "     - `[SEP]`: BERT uses this token to signify the end of a sentence, or between two sentences in sentence-pair tasks.\n",
    "\n",
    "   - **`token_type_ids`**: These IDs specify whether a token belongs to the first sentence (marked as 0) or the second sentence (marked as 1). In this case, the sentence is not part of a pair, so all the tokens are marked with `0`.\n",
    "     ```\n",
    "     'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "     ```\n",
    "\n",
    "   - **`attention_mask`**: This mask indicates which tokens should be attended to by the model. The value `1` means the token is a real token (not padding), and `0` would mean it's a padding token (in case the sentence was padded to a certain length). Since there are no padding tokens in this example, all the values are `1`.\n",
    "     ```\n",
    "     'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "     ```\n",
    "\n",
    "### What does each part mean?\n",
    "Let’s break it down based on each key of the dictionary:\n",
    "\n",
    "#### 1. **`input_ids`**:\n",
    "   These are the token IDs that represent the tokenized sentence. Each word, or subword (since BERT uses subword tokenization), is converted into its corresponding numeric ID from the BERT vocabulary. In this case, the sentence \"Our friends won't buy this analysis, let alone the next one we propose.\" is tokenized, with special tokens `[CLS]` and `[SEP]` added at the beginning and end, respectively.\n",
    "\n",
    "#### 2. **`token_type_ids`**:\n",
    "   These indicate which segment the token belongs to. Since BERT can handle sentence pairs, it needs to know which tokens belong to the first sentence (assigned 0) and which belong to the second sentence (assigned 1). Here, since we only have a single sentence, all the tokens are marked with 0.\n",
    "\n",
    "#### 3. **`attention_mask`**:\n",
    "   This mask is used to tell the model which tokens should be attended to and which tokens are padding. A value of 1 means that the model should pay attention to this token, and a value of 0 means it should ignore it (usually for padding tokens). In this case, all the tokens are relevant, so the entire attention mask is filled with 1s.\n",
    "\n",
    "### Summary of the tokenization process:\n",
    "- The sentence is split into tokens, each represented by an `input_id`.\n",
    "- Special tokens `[CLS]` and `[SEP]` are added to indicate the start and end of the sentence.\n",
    "- Each token is given a `token_type_id`, which is 0 for this single sentence.\n",
    "- An `attention_mask` is created to inform the model which tokens to focus on during processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] our friends won't buy this analysis, let alone the next one we propose. [SEP]\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(train_dataset[0]['sentence'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5946ed9e7047b88a689826aac13db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "        )\n",
    "\n",
    "train_dataset = train_dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a function `encode` and applies it to a dataset (`train_dataset`) using the `map` function from the Hugging Face datasets library. \n",
    "\n",
    "### 1. **The `encode` Function**:\n",
    "   ```python\n",
    "   def encode(examples):\n",
    "       return tokenizer(\n",
    "           examples[\"sentence\"],\n",
    "           truncation=True,\n",
    "           padding=\"max_length\",\n",
    "           max_length=512,\n",
    "       )\n",
    "   ```\n",
    "\n",
    "This function takes in a batch of examples from the dataset and tokenizes them using the `tokenizer`. Here's how it works:\n",
    "\n",
    "- **`examples[\"sentence\"]`**: This extracts the `\"sentence\"` field from the input batch. Presumably, `train_dataset` contains a column named `\"sentence\"` that holds the text data you want to tokenize.\n",
    "\n",
    "- **`truncation=True`**: This tells the tokenizer to truncate the sentence if its tokenized form exceeds a specified maximum length. In this case, it ensures that if the sentence is longer than `max_length=512`, it will be truncated to fit the limit.\n",
    "\n",
    "- **`padding=\"max_length\"`**: This ensures that each tokenized sequence is padded to the maximum length (`512` in this case). Padding tokens (`[PAD]`) will be added to the end of the sequence (or wherever specified) if it is shorter than 512 tokens. This makes sure that all tokenized examples are the same length.\n",
    "\n",
    "- **`max_length=512`**: This sets the maximum length of the tokenized sequences to 512 tokens. If a sentence is longer than 512 tokens, it will be truncated. If it’s shorter, padding tokens will be added to make it exactly 512 tokens long.\n",
    "\n",
    "- **Return value**: The tokenizer will return a dictionary of tokenized representations, including fields such as:\n",
    "  - `input_ids`: The list of token IDs corresponding to the input text.\n",
    "  - `attention_mask`: A mask that indicates which tokens are real and which are padding (1 for real tokens, 0 for padding).\n",
    "  - `token_type_ids`: If applicable, this distinguishes between different input sequences (but for single-sentence tasks, it’s all zeros).\n",
    "\n",
    "### 2. **Mapping the `encode` Function to the Dataset**:\n",
    "   ```python\n",
    "   train_dataset = train_dataset.map(encode, batched=True)\n",
    "   ```\n",
    "\n",
    "Here’s what this line does:\n",
    "\n",
    "- **`train_dataset.map(encode, batched=True)`**: This applies the `encode` function to each example (or batch of examples) in the `train_dataset`.\n",
    "  - **`batched=True`**: This tells the `map` function to process examples in batches rather than one at a time, which is more efficient. The `encode` function will receive a batch of sentences instead of a single sentence.\n",
    "  - The `map` function will apply the `encode` function to every example in the dataset, transforming each sentence into its tokenized form (with `input_ids`, `attention_mask`, etc.).\n",
    "\n",
    "After the `map` operation, `train_dataset` will contain the tokenized representations of the sentences, which are ready to be used in a model like BERT.\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1. **Batch of Examples**: The dataset is passed in batches (thanks to `batched=True`). For each batch, the `\"sentence\"` field is extracted.\n",
    "\n",
    "2. **Tokenization**: The `tokenizer` converts each sentence into token IDs and applies:\n",
    "   - **Truncation** if the sentence exceeds 512 tokens.\n",
    "   - **Padding** if the sentence is shorter than 512 tokens.\n",
    "   \n",
    "3. **Returned Tokenized Output**: The `tokenizer` returns a dictionary containing:\n",
    "   - `input_ids`: The tokenized form of the sentence.\n",
    "   - `attention_mask`: Indicates which tokens are real and which are padding.\n",
    "   - (Optional) `token_type_ids`: If needed, distinguishes between sentence segments.\n",
    "\n",
    "4. **Dataset Update**: After applying `map`, the `train_dataset` now contains the tokenized versions of each sentence, ready to be passed to a BERT model or another transformer-based model for training or inference.\n",
    "\n",
    "### Example Output:\n",
    "For a single sentence, the dataset will now look something like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'input_ids': [101, 2026, 2814, ..., 102, 0, 0, ..., 0],  # Padded/truncated to 512 tokens\n",
    "  'attention_mask': [1, 1, 1, ..., 1, 0, 0, ..., 0],  # 1s for real tokens, 0s for padding\n",
    "  'token_type_ids': [0, 0, 0, ..., 0],  # All 0s if single sentence task\n",
    "}\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "- The `encode` function tokenizes the sentences from the dataset.\n",
    "- It applies truncation if the sentence is longer than 512 tokens.\n",
    "- It pads the sentence to 512 tokens if it is shorter.\n",
    "- The `map` function applies this transformation to every sentence in the dataset in batches, converting the raw text into a format that can be fed into a transformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1]),\n",
       " 'input_ids': tensor([[  101,  2256,  2814,  ...,     0,     0,     0],\n",
       "         [  101,  2028,  2062,  ...,     0,     0,     0],\n",
       "         [  101,  2028,  2062,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  5965, 12808,  ...,     0,     0,     0],\n",
       "         [  101,  2198, 10948,  ...,     0,     0,     0],\n",
       "         [  101,  3021, 24471,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([7, 512]) torch.Size([7, 512]) torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['label'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
